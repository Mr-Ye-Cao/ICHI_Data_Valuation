\section{Conclusion}

We apply scalable gradient-based attribution (LARK) to investigate knowledge utilization in LLMs fine-tuned for biomedical QA. Our experiments reveal a ``knowledge awakening'' phenomenon: pretrained knowledge influence increases monotonically during fine-tuning (0\% to 28.4\% dominance), suggesting fine-tuning unlocks rather than overwrites pretrained domain knowledge. The method correctly distinguishes domain-relevant from irrelevant data and outperforms lexical methods like BM25 by 46.2 percentage points. These findings enable practitioners to audit knowledge sources driving clinical predictions and inform optimal fine-tuning strategies for medical AI.

% \section{Acknowledgment}
% The work of Zhaozhuo Xu was supported by NSF grants 2451398 and 2450524. 






