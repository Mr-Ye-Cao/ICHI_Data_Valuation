\section{Related Work}

\noindent\textbf{Training Data Attribution.} Understanding which training samples influence model predictions has been studied through influence functions~\cite{koh2017understanding}, which measure the effect of upweighting individual samples on model parameters. While theoretically principled, classical influence functions require Hessian inversion, making them computationally prohibitive for large models. TracIn~\cite{pruthi2020estimating} approximates influence by summing gradient dot products across training checkpoints, but still requires storing full gradients. Recent work on scalable attribution includes RapidIn~\cite{lin2024rapidin}, which uses random projection (OPORP)~\cite{li2023oporp} to compress gradients while preserving inner product estimates, enabling attribution at billion-parameter scale.

\noindent\textbf{LLMs in Healthcare.} Large language models have shown promise in biomedical applications including clinical decision support, medical question answering, and literature analysis. Models like Med-PaLM~\cite{singhal2023large} and GPT-4 achieve strong performance on medical benchmarks such as USMLE and PubMedQA~\cite{jin2019pubmedqa}. However, deploying LLMs in healthcare raises concerns about transparency and accountability, as practitioners need to understand what knowledge drives model predictions. Our work addresses this gap by providing tools to trace predictions back to their training data sources.

\noindent\textbf{Fine-tuning Dynamics.} Prior work has studied how neural networks learn during training, including the lottery ticket hypothesis~\cite{frankle2019lottery} and neural network pruning. For LLMs, research has examined how fine-tuning affects pretrained representations and whether fine-tuning overwrites or builds upon pretrained knowledge. Our work contributes to this understanding by quantitatively measuring the relative influence of pretrained versus fine-tuned data throughout the fine-tuning process, revealing a progressive ``unlocking'' of pretrained knowledge.