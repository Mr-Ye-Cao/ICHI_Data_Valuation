\section{Related Works}


Recent advances in model compression have prioritized making LLMs more accessible to resource-constrained environments through techniques such as quantization~\cite{liu2024kivi,zhang2024nomad,zhang2024kv} and sparsification~\cite{zhou2024sirius,guo2025zo,Wu2025}. However, these methods often induce performance degradation, especially on complex reasoning tasks. \cite{xu2025compress} introduced the concept of compression-aware computing, advocating for LLM systems that can recognize and adapt to their compressed states. This framework emphasizes the integration of compression techniques with model introspection to preserve performance while enhancing efficiency—particularly relevant for domains requiring scalable and sustainable AI, such as biomedical research~\cite{wu2024weighted}.

Complementing this direction, \cite{xu2024soft} proposed a surprisingly simple yet effective strategy: using soft prompt tuning to recover performance in heavily compressed LLMs. Their findings demonstrate that learned prompts can significantly restore task performance across quantized and pruned models. More importantly, these prompts exhibit transferability across tasks, compression levels, and even model architectures—subverting the conventional belief that prompt tuning is inherently task-specific. This line of work suggests that LLMs, though altered by compression, retain latent capacities that can be accessed or enhanced through targeted prompting.