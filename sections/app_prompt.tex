\section{Prompt Template at Different Design Phases}\label{app:design phases}
\Zhaozhuo{This is just an example of putting prompt in the paper.}
\textbf{Prompt after Phase 1 (The Initial Prompt):}



\begin{lstlisting}[style=promptcode]
def generate_prompt(premise, choice1, choice2, question, label):
    # Dynamically insert the correct answer based on the label
    correct_choice = choice1 if label == 0 else choice2
    return f"""
    You are tasked with rephrasing the following premise while preserving its original meaning and ensuring that it remains consistent with its associated question and choices. The rephrased premise must maintain the logic of the original data so that the correct answer remains unchanged. The rephrased data will be used to fine-tune a memory-efficient zeroth-order optimizer (MeZO) model. Below are the ABSTRACT and INTRODUCTION sections from the MeZO research paper to provide additional context:

    ABSTRACT:
    "Fine-tuning language models (LMs) has yielded success on diverse downstream tasks, but as LMs grow in size, backpropagation requires a prohibitively large amount of memory. Zeroth-order (ZO) methods can in principle estimate gradients using only two forward passes but are theorized to be catastrophically slow for optimizing large models. In this work, we propose a memory-efficient zeroth-order optimizer (MeZO), adapting the classical ZO-SGD method to operate in-place, thereby fine-tuning LMs with the same memory footprint as inference. For example, with a single A100 80GB GPU, MeZO can train a 30-billion parameter model, whereas fine-tuning with backpropagation can train only a 2.7B LM with the same budget. Our results demonstrate that (1) MeZO significantly outperforms in-context learning and linear probing; (2) MeZO achieves comparable performance to fine-tuning with backpropagation across multiple tasks, with up to 12× memory reduction and up to 2× GPU-hour reduction in our implementation; (3) MeZO is compatible with both full-parameter and parameter-efficient tuning techniques such as LoRA and prefix tuning; (4) MeZO can effectively optimize non-differentiable objectives (e.g., maximizing accuracy or F1). We support our empirical findings with theoretical insights, highlighting how adequate pre-training and task prompts enable MeZO to fine-tune huge models, despite classical ZO analyses suggesting otherwise."
    
    INTRODUCTION:
    "Fine-tuning pre-trained language models (LMs) has been the dominant methodology for solving many language tasks, adapting to specialized domains, or incorporating human instructions and preferences. However, as LMs are scaled up, computing gradients for backpropagation requires a prohibitive amount of memory – in our test, up to 12× the memory required for inference – because it needs to cache activations during the forward pass, gradients during the backward pass, and, in the case of Adam, also store gradient history (see Section 3.4 for a detailed analysis). As a result, while it is possible to run inference with a 30-billion (30B) parameter LM on a single Nvidia A100 GPU (with 80GB memory), backpropagation with Adam is feasible only for a 2.7B LM. Parameter-efficient fine-tuning methods (PEFT) update just a fraction of the network parameters but still need to cache many activations, because the tuned parameters are scattered throughout the model. In our tests, fine-tuning an OPT-13B model with full parameter tuning or PEFT requires 12× and 6× more memory than inference respectively."
    
    FIGURE 1: This figure presents the performance of the OPT-13B model across multiple tasks (SST-2, RTE, CB, BoolQ, WSC, WiC, MultiRC, Copa, ReCoRD, SQuAD, and DROP) under four configurations:
    1. Zero-shot: Where no task-specific training is performed, and the model is tested directly.
    2. In-context learning (ICL): Where labeled examples are provided in the input prompt.
    3. MeZO fine-tuning: Using the MeZO optimizer with LoRA or prefix-tuning.
    4. Full fine-tuning (FT): Using Adam optimizer with full parameter tuning, which consumes 12× memory.
    
    Key observations from Figure 1:
    - MeZO achieves superior results over zero-shot and in-context learning.
    - MeZO’s performance is comparable to full fine-tuning, with less than 1% accuracy or F1 difference on 7 out of 11 tasks.
    - MeZO requires only 1/12th the memory compared to full fine-tuning.
    
    Backpropagation also cannot optimize non-differentiable criteria, which have gained popularity in fine-tuning LMs according to discrete metrics or set safety standards. Typically, these adaptations involve expensive reinforcement learning from human feedback (RLHF). A classical zeroth-order optimization method, ZO-SGD, uses only differences of loss values to estimate the gradients. Thus, in principle, the method can update neural networks with just forward passes, though naive implementation still doubles the memory overhead and scales poorly with model size. Existing ZO methods have been applied in deep learning settings to find adversarial examples or tune input embeddings [91, 90] but not to directly optimize large-scale models (see Liu et al. for a survey).
    
    MeZO’s Contributions:
    1. In MeZO, we adapt the ZO-SGD algorithm and a number of variants to operate in-place on arbitrarily large models with almost no memory overhead (see Algorithm 1 and Section 2).
    2. We conduct comprehensive experiments across model types (masked LM and autoregressive LM), model scales (from 350M to 66B), and downstream tasks (classification, multiple-choice, and generation). MeZO consistently outperforms zero-shot, ICL, and linear probing. With RoBERTa-large, MeZO achieves performance close to standard fine-tuning within 5%. With OPT-13B, MeZO outperforms or performs comparably to fine-tuning on 7 out of 11 tasks, despite requiring roughly 12× less memory (Figure 1 and Section 3). In our implementation, MeZO requires only half as many GPU-hours as Adam fine-tuning for a 30B model (see Appendix F.6).
    3. We demonstrate MeZO’s compatibility with full-parameter tuning and PEFT (e.g., LoRA and prefix-tuning) in Section 3.
    4. Further exploration showcases that MeZO can optimize non-differentiable objectives such as accuracy or F1 score while still requiring only the same memory as inference (Section 3.3).
    5. Our theory suggests that adequate pre-training ensures a better optimization rate (Theorem 1) and a stable convergence rate (Lemma 3.4) of MeZO under proper settings of hyperparameters. This matches practical experience, with Assumption 3.1 specifying sparsity of parameters."
    
    ALGORITHM 1: The MeZO algorithm uses perturbation-based gradient estimation:
    1. Parameters are perturbed to estimate gradients using forward passes only.
    2. Parameters are updated in place, avoiding memory overhead from gradient storage.
    3. Memory efficiency is achieved, requiring only inference-level resources.

    ---
    
    Your task: Rephrase the given premise without altering its meaning. The rephrased premise must remain consistent with the provided question and choices, such that the correct answer remains unchanged.
    
    Original premise: "{premise}"  
    Choice 1: "{choice1}"  
    Choice 2: "{choice2}"  
    Question type: "{question}" (cause or effect)  
    Correct answer: "{correct_choice}"  
    
    Directly output only one rephrased sentence without any other characters and other explanatory statements like "The rephrased sentence is:":
    """
\end{lstlisting}